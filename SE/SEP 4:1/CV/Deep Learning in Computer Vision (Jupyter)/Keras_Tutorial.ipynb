{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 13016370 Computer Vision\n",
    "# Deep Learning in Computer Vision: A Very Breif Introduction\n",
    "###### by Ukrit Watchareeruetai, Software Engineering Program, International College, KMITL\n",
    "###### Draft: November 12, 2018\n",
    "\n",
    "- **Objective:** To explain the concept of deep learning and how it is applied to computer vision\n",
    "- **Contents:**\n",
    "  - Pattern recognition, machine learning, and deep learning\n",
    "  - Building and training neural networks using Keras package\n",
    "    - Loading a dataset\n",
    "    - Building a simple feedforward neural network\n",
    "    - Building a convolutional neural network\n",
    "  - Case study: Pokemon classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pattern recognition\n",
    "\n",
    "- An approach of computer vision to perform image/object recognition is based on the concept of **pattern recognition**.\n",
    "- This approach can be divided into two main steps:\n",
    "    - **Feature extraction**\n",
    "      - Measure some distinct characteristics of those objects\n",
    "      - Convert those characteristics into numerical values\n",
    "      - These measured characteristics are called **features**.\n",
    "      - Several features of an object can be grouped into one vector, a.k.a., **feature vector**.\n",
    "      - **Preprocessing** might be required to convert an input into a more proper form.\n",
    "        - Point-processing, local processing\n",
    "        - Frequency-domain method\n",
    "        - Image reconstruction/enhancement\n",
    "        - Image segmentation\n",
    "    - **Classification**\n",
    "      - Create some rules for distinguishing one type of objects from another based upon its feature vector.\n",
    "      - A mathematical model incorporating these rules is referred to as a **classifier**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Cat and dog recognition\n",
    "\n",
    "- Features:\n",
    "  - Feature 1 ($f_1$): body length\n",
    "  - Feature 2 ($f_2$): skull length\n",
    "\n",
    "<img src=\"figures/feature_extraction.png\" width=\"60%\">\n",
    "\n",
    "Images from the Internet: These Photos by Unknown Author is licensed under CC BY-SA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature spcae\n",
    "\n",
    "- A space of features used to represent objects\n",
    "  - In this case, it is a space of two features:\n",
    "    - Body length\n",
    "    - Skull length\n",
    "  - In practical, $n$-dimensional of feature spcae is considered.\n",
    "    - $n$ is the number of features.\n",
    "\n",
    "<img src=\"figures/feature_space.png\" width=\"60%\">\n",
    "\n",
    "Images from the Internet: These Photos by Unknown Author is licensed under CC BY-SA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification rule\n",
    "\n",
    "- A rule used to classify each instance into a class based upon the features\n",
    "  - Three examples of linear decision rule\n",
    "\n",
    "<img src=\"figures/classification_rule_1.png\" width=\"60%\">\n",
    "<img src=\"figures/classification_rule_2.png\" width=\"60%\">\n",
    "<img src=\"figures/classification_rule_3.png\" width=\"60%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification rule\n",
    "\n",
    "- Using only **one instance** (a.k.a., **data sample**) from each class cannot create a robust rule:\n",
    "- Need more data instances!\n",
    "\n",
    "<img src=\"figures/feature_extraction_more_instances.png\" width=\"60%\">\n",
    "\n",
    "Images from the Internet: These Photos by Unknown Author is licensed under CC BY-SA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification rule\n",
    "\n",
    "- The previous classification rules do not work perfectly any more.\n",
    "- Cannot correctly classify some instances\n",
    "- A set of data instances used in the process of classification construction is called a **training dataset**.\n",
    "- The process of constructing a classifier is known as **training** or **learning**.\n",
    "- To assess the performance of a learned model, another set of data, known as a **test set**, hould be used.\n",
    "  - This test set is used to evaluate the **generalization performance** of the model.\n",
    "    - How well the model can be applied to predict/classify unseen patterns\n",
    "\n",
    "<img src=\"figures/classification_rules_fail.png\" width=\"60%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear classifier\n",
    "\n",
    "- A new rule can be created.\n",
    "  - This classification rule is **linear**.\n",
    "  - A classifier obeying this linear classification rule is called a **linear classifier**.\n",
    "  - Its decision boundary can be represented in the form:\n",
    "    - $c_0 + c_1f_1 + c_2f_2 + … + c_nf_n = 0$\n",
    "    - Let $n$ denote the number of features\n",
    "    - **Parameters** $c_i$ control the shape of decision rule.\n",
    "  - However, in practical, the distributions of objects in the feature space are usually not **linearly separable**.\n",
    "    - Linearly separable problem means it can be solved perfectly using a linear classifier.\n",
    "      - Can be perfectly classify by a linear decision rule\n",
    "      - 2D cases: A straight line\n",
    "      - 3D cases: A flat surface known as a plane\n",
    "      - 4D or higher cases: A hyperplane\n",
    "  \n",
    "<img src=\"figures/linear_classifier.png\" width=\"60%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-linear classifier\n",
    "\n",
    "- A **non-linear classifier** classifies data using a more complicated rule than a hyperplane decision rule.\n",
    "  - Construct a non-linear decision rule\n",
    "\n",
    "- The term **capacity** relates to the complexity of the model that can be created.\n",
    "  - High capacity -> complex model\n",
    "  - Low capacity -> simple model\n",
    "\n",
    "- A classifier model with too high capacity tends to **overfit** to the training dataset.\n",
    "  - A classifier is said to be overfit if it can successfully classify a training dataset but not fails to do so for an unknown test set.\n",
    "  - It cannot capture the true nature of the problem.\n",
    "    - It learns something irrelevant to the problem but coincidentally presented in most instances in the training set.\n",
    "\n",
    "<img src=\"figures/non_linear_classifier_1.png\" width=\"60%\">\n",
    "<img src=\"figures/non_linear_classifier_2.png\" width=\"60%\">\n",
    "<img src=\"figures/non_linear_classifier_3.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning\n",
    "\n",
    "- **Machine learning** is a field of AI studying how to make a computer learns some rules for solving a problem from given data.\n",
    "- It can be used to adjust the parameters of a classifier or decision rule based upon a given training dataset.\n",
    "  - Let the computer learn the rules themselve\n",
    "  - Require less human intervention\n",
    "    - For a very complex problem, human might not be able to construct a set of rules effectively.\n",
    "    - Machine can do it better!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial neural network\n",
    "\n",
    "- **Artificial neural network (ANN)** or in short **neural network (NN)** is a classfication model inspired by biological neurons in the brain.\n",
    "  - Usually consist of two or more layers of neurons\n",
    "  - Three fundamental types of layer:\n",
    "    - **Input layer**\n",
    "      - Directly connect to the input of the network\n",
    "      - The number of neurons in this layer equals to the number of features.\n",
    "    - **Output layer**\n",
    "      - The output of the network\n",
    "      - For classification problems, the number of output neurons equals to the number of classes.\n",
    "    - **Hidden layer**\n",
    "      - Layers between the input and output layers\n",
    "      - Transform input features into a more complicated form\n",
    "      \n",
    "  - The following figure is an example of a **fully-connected**, **feedforward** neural network.\n",
    "    - _Fully-connected_ means each neuron in a layer is connected to each neuron in the adjacent layer(s).\n",
    "    - _Feedforward_ means there is no backward connection. The data is feeded-forward from the input layer, through the hidden layer, to the output layer.\n",
    "  \n",
    "<img src=\"figures/nn_layers.png\" width=\"60%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How neural networks work?\n",
    "\n",
    "- Each neuron has some connections to other neurons, usually in the adjacent layers.\n",
    "- There is a numerical value called **weight** associated to each connection in the network.\n",
    "- The weight determines the importance of each connection.\n",
    "  - Higher weight -> more important\n",
    "  - Smaller weight -> less important\n",
    "- Each neurons in the hidden layer recieves inputs from the previous layer.\n",
    "- Then perform a calcuation as follows:\n",
    "\n",
    "  \\begin{equation}\n",
    "  h_i = g(w_{i1}x_1 + w_{i2}x_2 + \\cdots + w_{iD}x_D + b_i)\n",
    "  \\end{equation}\n",
    "\n",
    "  - $g(\\cdot)$ is an **activation function**.\n",
    "    - Introduce non-linearity to the network\n",
    "  - $w_{ij}$ represents the weight of the connection between the node $j$ and the input $i$.\n",
    "  - $b_i$ is known as **bias**.\n",
    "    - A numerical value associated to each hidden neuron (and output neurons in some models)\n",
    "  - Both wegiths and bias are parameters of the model that need to be adjusted by a **training/learning algorithm**.\n",
    "- The output of a hidden neuron is sent to all connected neurons in the next layers.\n",
    "\n",
    "<img src=\"figures/nn_weight.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to train neural networks?\n",
    "\n",
    "- A traditional learning algorithm for neural networks is called **gradient based learing**.\n",
    "  - A sample or batch of training data is feeded through the network to produce the output.\n",
    "  - The output is then compared with a known answer, called **target** or **label**.\n",
    "  - By using the target, it can be judged if the output is correct or wrong.\n",
    "  - If the output is correct, just do nothing.\n",
    "  - If the output is wrong, the **error** or **loss** will be used to adjust the weights and biases of the network.\n",
    "    - The error is a function of parameters, i.e., weights and biases.\n",
    "    - The error is differentiated with respect to each weight and each bias.\n",
    "      - The result of the differentation is known as the **gradient vector**.\n",
    "    - To minimize the error, the weights and biases will be adjusted in the direction of opposite to the gradient vector.\n",
    "    - This technique is known as **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning\n",
    "\n",
    "- **Deep learning** is a subfield of machine learning aiming to study about **deep neural networks** and their training/learning algorithms.\n",
    "  - A deep neural network refers to a neural network consisting of many layers.\n",
    "    - In the past, a network with 4-6 layers was considered _deep_.\n",
    "    - Presently, neural networks may consist of more than 100 layers.\n",
    "\n",
    "- A deep neural network can automatically build more complicated, useful features from layer-to-layer.\n",
    "  - It has been shown in many papers that **feature engineering** is not required.\n",
    "  - **Hand-crafted features** become no longer significant.\n",
    "  - A deep neural network can be trained directly from raw data.\n",
    "  \n",
    "- However, a deep neural network needs to be tuned carefully and trained with some well-designed technqiues.\n",
    "  - In the past, deep neural networks could not gain popularity from researchers because there were no effective learning algorihtms.\n",
    "    - A well-known problem in training a deep neural network is known as **gradient vanishing problem**.\n",
    "  - So far, several techniques that enable learning deep neural networks have been proposed.\n",
    "    - Pretraining\n",
    "    - Dropout\n",
    "    - Regularization\n",
    "    - Weight sharing\n",
    "- Another drawback of deep learning is that it usually requires a huge amount of training data.\n",
    "  - Inadequate amount of training data leads to overfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building and training neural networks using Keras\n",
    "- Keras is a high-level deep learning library written in Python\n",
    "- Built on top on other libraries:\n",
    "  - TensorFlow\n",
    "  - Theano\n",
    "  - CNTK\n",
    "- Allow for easy prototyping and fast experimentation\n",
    "- Installation\n",
    "  - `pip install keras`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading dataset using Keras\n",
    "- Keras provides a module for loading various benchmark computer vision datasets:\n",
    "  - MNIST:\n",
    "    - 28 $\\times$ 28 grayscale images (784 features)\n",
    "    - 10 classes of handwritten digit (0-9)\n",
    "    - 60,000 training samples\n",
    "    - 10,000 test samples\n",
    "    - <http://yann.lecun.com/exdb/mnist/>\n",
    "  - Fasion-MNIST: \n",
    "    - Same as MNIST but different problem-domain\n",
    "    - 10 classes of fashion images\n",
    "      - T-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot\n",
    "    - <https://www.kaggle.com/zalando-research/fashionmnist/home>\n",
    "  - CIFAR-10: \n",
    "    - 32 $\\times$ 32 color images (3 $\\times$ 32  $\\times$  32 = 3,072 features)\n",
    "    - 10 classes\n",
    "      - Airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks \n",
    "    - 50,000 training samples\n",
    "    - 10,000 test samples\n",
    "    - <https://www.cs.toronto.edu/~kriz/cifar.html>\n",
    "  - CIFAR-100:\n",
    "    - Same as CIFAR-10 but have more categories\n",
    "    - 100 classes grouped into 20 superclasses\n",
    "    - <https://www.cs.toronto.edu/~kriz/cifar.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukrit\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "#(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "#(X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(X_train[100,:].reshape((28,28)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(y_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One-hot encoding\n",
    "- One-hot encoding (OHE) is often used as a target vector for neural networks.\n",
    "- The size of the vector equals the number of classes.\n",
    "- All elements are 0 except the element corresponding to the label, which is 1.\n",
    "- Each element is the target of each neuron in the output layer.\n",
    "- For examples:\n",
    "    - $[1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0]$ = class 0 \n",
    "    - $[0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0]$ = class 1 \n",
    "    - $[0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0]$ = class 2 \n",
    "    - $[0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1]$ = class 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "print(y_train[100])\n",
    "print(Y_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a simple model\n",
    "- A sequential model\n",
    "  - Create a feed-forward neural networks\n",
    "     - No backward connections allowed\n",
    "     - Just stack layers on top of another\n",
    "- Example: creating a feed-forward NN with the following specfications:\n",
    "  - Input: 784 nodes\n",
    "  - No hidden layer\n",
    "  - No activation\n",
    "  - Output: 10 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking the model\n",
    "- Before executing the model, it is worth to check its structure\n",
    "- Can be done using method: `model.summary()`\n",
    "- Describe each layer in the model, shape of output, and the number of weights (parameters)\n",
    "  - Bias is included in a Dense layer automatically by adding an extra constant input (its value is 1).\n",
    "  - There is a connection between this constant input to each node in the Dense layer.\n",
    "  - The value associated to the connection is the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compile and fit the model\n",
    "- The model needs to be trained before it can be used.\n",
    "- Specify loss function and learning algorithm by `model.complie()`\n",
    "  - Loss function\n",
    "    - Mean squared error (`loss='mean_squared_error'`)\n",
    "      \\begin{equation}\n",
    "      MSE = \\frac{1}{S} \\sum_{s=1}^{S} \\Vert \\mathbf{t}_s - \\mathbf{o}_s \\Vert_2^2\n",
    "      \\end{equation}\n",
    "      - $S$ is the total number of samples used to calculate the loss     \n",
    "      - $\\mathbf{o}_s$ is the output produced by the network given the $s$-th input sample\n",
    "      - $\\mathbf{t}_s$ is the target of the $s$-th input sample\n",
    "         \n",
    "    - Mean absolute error (`loss='mean_absolute_error'`)\n",
    "      \\begin{equation}\n",
    "      MAE = \\frac{1}{S} \\sum_{s=1}^{S} \\vert \\mathbf{t}_s - \\mathbf{o}_s \\vert\n",
    "      \\end{equation}\n",
    "    \n",
    "    - Categoriacal cross-entropy (`loss='categorical_crossentropy'`)\n",
    "      \\begin{equation}\n",
    "      CCE = -\\frac{1}{S} \\sum_{s=1}^{S} \\sum_{c=1}^{C} \\mathbf{t}_s \\cdot log(\\mathbf{p}_s) \\\\\n",
    "      \\mathbf{p}_s = softmax(\\mathbf{o}_s)\n",
    "      \\end{equation}\n",
    "       \n",
    "  - Optimization algorithm\n",
    "    - Stochastic gradient descent (SGD)\n",
    "    - RMSprop\n",
    "    - Adam\n",
    "    - Adagrad\n",
    "    - AdaMax\n",
    "  - Metrics\n",
    "    - Accuracy\n",
    "      - The ratio between the number of correctly classified samples and the total number of samples\n",
    "        \n",
    "- Train the model using `model.fit()`\n",
    "  - Training samples\n",
    "  - Corresponding labels\n",
    "  - Batch size\n",
    "  - Number of epochs\n",
    "  - Ratio of validation set\n",
    "  - Progress message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 6.9737 - acc: 0.2028 - val_loss: 7.8976 - val_acc: 0.1853\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 8.1615 - acc: 0.1570 - val_loss: 8.5040 - val_acc: 0.1048 - loss: 8.0808 - acc: 0.1\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 8.5623 - acc: 0.1002 - val_loss: 9.2528 - val_acc: 0.1057\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 9.1896 - acc: 0.1079 - val_loss: 9.6341 - val_acc: 0.1129\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 9.2071 - acc: 0.1059 - val_loss: 9.6354 - val_acc: 0.1122\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 9.1343 - acc: 0.1075 - val_loss: 9.7195 - val_acc: 0.1119\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 9.5013 - acc: 0.1077 - val_loss: 9.6410 - val_acc: 0.1146\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 9.1728 - acc: 0.1153 - val_loss: 10.1832 - val_acc: 0.1152\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 10.4728 - acc: 0.1117 - val_loss: 10.7153 - val_acc: 0.1198\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 10.6977 - acc: 0.1169 - val_loss: 11.1074 - val_acc: 0.1219\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 10.9027 - acc: 0.1182 - val_loss: 11.1511 - val_acc: 0.1231\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 11.0022 - acc: 0.1168 - val_loss: 11.3055 - val_acc: 0.1217\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 11.1512 - acc: 0.1162 - val_loss: 11.4384 - val_acc: 0.1210\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 11.2466 - acc: 0.1160 - val_loss: 11.4504 - val_acc: 0.1219\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 11.2341 - acc: 0.1155 - val_loss: 11.4724 - val_acc: 0.1208\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 11.2254 - acc: 0.1159 - val_loss: 11.4961 - val_acc: 0.1223\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 11.1130 - acc: 0.1185 - val_loss: 11.2730 - val_acc: 0.1259\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 11.0513 - acc: 0.1203 - val_loss: 11.3496 - val_acc: 0.1258\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 10.9956 - acc: 0.1193 - val_loss: 10.4790 - val_acc: 0.1216\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 10.9885 - acc: 0.1151 - val_loss: 11.3156 - val_acc: 0.1213\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 11.0932 - acc: 0.1148 - val_loss: 11.3354 - val_acc: 0.1215\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 11.0803 - acc: 0.1148 - val_loss: 11.2829 - val_acc: 0.1212\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 11.0688 - acc: 0.1148 - val_loss: 11.3107 - val_acc: 0.1216\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 11.0184 - acc: 0.1150 - val_loss: 11.1128 - val_acc: 0.1228\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 10.7908 - acc: 0.1161 - val_loss: 11.2955 - val_acc: 0.1233\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 10.6860 - acc: 0.1176 - val_loss: 11.0731 - val_acc: 0.1253\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 11.0980 - acc: 0.1155 - val_loss: 11.3502 - val_acc: 0.1222\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 11.1069 - acc: 0.1142 - val_loss: 11.1743 - val_acc: 0.1222\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 10.6609 - acc: 0.1146 - val_loss: 10.3443 - val_acc: 0.1221\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 10.8082 - acc: 0.1149 - val_loss: 11.1906 - val_acc: 0.1222\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 10.3626 - acc: 0.1125 - val_loss: 10.0929 - val_acc: 0.1218\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 8.3167 - acc: 0.1150 - val_loss: 9.7081 - val_acc: 0.1231\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 9.9644 - acc: 0.1155 - val_loss: 10.2090 - val_acc: 0.1239\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 10.1150 - acc: 0.1178 - val_loss: 10.2702 - val_acc: 0.1266\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 10.1490 - acc: 0.1195 - val_loss: 10.2799 - val_acc: 0.1277\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 10.1055 - acc: 0.1209 - val_loss: 9.4033 - val_acc: 0.1300\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 9.2463 - acc: 0.1243 - val_loss: 9.8748 - val_acc: 0.1320\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 10.0705 - acc: 0.1248 - val_loss: 10.2970 - val_acc: 0.1321\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 10.4129 - acc: 0.1249 - val_loss: 10.4949 - val_acc: 0.1322\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 10.6043 - acc: 0.1249 - val_loss: 10.6637 - val_acc: 0.1321\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 10.6986 - acc: 0.1249 - val_loss: 10.7668 - val_acc: 0.1322\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 10.7866 - acc: 0.1249 - val_loss: 10.8377 - val_acc: 0.1322\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 10.8452 - acc: 0.1250 - val_loss: 10.9020 - val_acc: 0.1322\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 10.8890 - acc: 0.1250 - val_loss: 10.9341 - val_acc: 0.1322\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 10.9309 - acc: 0.1250 - val_loss: 10.9730 - val_acc: 0.1322\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 11.0101 - acc: 0.1249 - val_loss: 11.0867 - val_acc: 0.1322\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 11.0652 - acc: 0.1249 - val_loss: 11.1727 - val_acc: 0.1322\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 10.5086 - acc: 0.1250 - val_loss: 10.8328 - val_acc: 0.1322\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 10.6326 - acc: 0.1268 - val_loss: 8.4302 - val_acc: 0.1450\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 9.5287 - acc: 0.1348 - val_loss: 10.4135 - val_acc: 0.1375\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 10.7901 - acc: 0.1267 - val_loss: 11.1131 - val_acc: 0.1324\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 9.6579 - acc: 0.1284 - val_loss: 10.0217 - val_acc: 0.1347\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 8.5048 - acc: 0.1324 - val_loss: 8.6444 - val_acc: 0.1414\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 8.5430 - acc: 0.1321 - val_loss: 8.6875 - val_acc: 0.1382\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 8.9086 - acc: 0.1331 - val_loss: 9.3117 - val_acc: 0.1428\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 9.1687 - acc: 0.1359 - val_loss: 9.2764 - val_acc: 0.1423\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 9.7399 - acc: 0.1343 - val_loss: 10.1005 - val_acc: 0.1395\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 9.9923 - acc: 0.1309 - val_loss: 10.3086 - val_acc: 0.1376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 10.2586 - acc: 0.1308 - val_loss: 10.0063 - val_acc: 0.1385\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 10.2399 - acc: 0.1304 - val_loss: 10.3887 - val_acc: 0.1383\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 10.4342 - acc: 0.1301 - val_loss: 11.3428 - val_acc: 0.1361\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 10.9273 - acc: 0.1290 - val_loss: 10.4079 - val_acc: 0.1378\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 9.8884 - acc: 0.1268 - val_loss: 8.0108 - val_acc: 0.1181\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 8.0649 - acc: 0.1147 - val_loss: 8.3250 - val_acc: 0.1187\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 8.4352 - acc: 0.1163 - val_loss: 8.5883 - val_acc: 0.1220\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 8.6869 - acc: 0.1188 - val_loss: 8.8076 - val_acc: 0.1246\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 8.7276 - acc: 0.1224 - val_loss: 8.8635 - val_acc: 0.1302\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 8.6686 - acc: 0.1268 - val_loss: 8.7539 - val_acc: 0.1328\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 8.6927 - acc: 0.1283 - val_loss: 8.7379 - val_acc: 0.1329\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 8.5290 - acc: 0.1270 - val_loss: 8.4604 - val_acc: 0.1283\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 8.4590 - acc: 0.1254 - val_loss: 8.5652 - val_acc: 0.1284\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 9.0594 - acc: 0.1301 - val_loss: 9.6282 - val_acc: 0.1408\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 10.3560 - acc: 0.1407 - val_loss: 10.9688 - val_acc: 0.1465\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 11.0260 - acc: 0.1376 - val_loss: 11.1194 - val_acc: 0.1445\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 9.9624 - acc: 0.1406 - val_loss: 9.3041 - val_acc: 0.1513\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 9.1349 - acc: 0.1434 - val_loss: 9.6927 - val_acc: 0.1497\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 9.5910 - acc: 0.1429 - val_loss: 9.4589 - val_acc: 0.1497\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 9.7426 - acc: 0.1440 - val_loss: 9.9542 - val_acc: 0.1513\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 10.8537 - acc: 0.1398 - val_loss: 10.3912 - val_acc: 0.1493\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 10.1563 - acc: 0.1394 - val_loss: 7.0288 - val_acc: 0.1413\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 9.8666 - acc: 0.1337 - val_loss: 10.3360 - val_acc: 0.1414\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 10.3411 - acc: 0.1338 - val_loss: 10.2796 - val_acc: 0.1414\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 10.2313 - acc: 0.1340 - val_loss: 10.2189 - val_acc: 0.1419\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 10.1644 - acc: 0.1344 - val_loss: 10.1086 - val_acc: 0.1423\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 9.3607 - acc: 0.1348 - val_loss: 8.4048 - val_acc: 0.1433\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 9.6091 - acc: 0.1371 - val_loss: 9.6100 - val_acc: 0.1468\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 9.7851 - acc: 0.1390 - val_loss: 9.6178 - val_acc: 0.1478\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 9.7518 - acc: 0.1399 - val_loss: 9.6113 - val_acc: 0.1484\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 9.6081 - acc: 0.1403 - val_loss: 9.1107 - val_acc: 0.1488\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 9.5280 - acc: 0.1405 - val_loss: 9.6879 - val_acc: 0.1490\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 9.8765 - acc: 0.1409 - val_loss: 9.8529 - val_acc: 0.1493\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 9.9951 - acc: 0.1413 - val_loss: 9.9118 - val_acc: 0.1497\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 10.0356 - acc: 0.1418 - val_loss: 9.9238 - val_acc: 0.1499\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 10.0642 - acc: 0.1421 - val_loss: 9.9773 - val_acc: 0.1501\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 10.1002 - acc: 0.1424 - val_loss: 9.9736 - val_acc: 0.1504\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 10.0496 - acc: 0.1431 - val_loss: 10.1322 - val_acc: 0.1517\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 10.2743 - acc: 0.1453 - val_loss: 10.0159 - val_acc: 0.1528\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 10.1129 - acc: 0.1461 - val_loss: 10.0983 - val_acc: 0.1533\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 10.2007 - acc: 0.1468 - val_loss: 9.8267 - val_acc: 0.1536\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 10.0198 - acc: 0.1472 - val_loss: 10.0824 - val_acc: 0.1541\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size=200, epochs=100, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate the trained model\n",
    "- Use method `model.evaluate()`\n",
    "- Pass an unseen dataset to evaluate the generalization performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 61us/step\n",
      "Test score: 10.133972030639649\n",
      "Test accuracy: 0.1548\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Try again\n",
    "- Normalize input data\n",
    "- Add a hidden layer with an activation function\n",
    "- Use softmax activation function at the output node\n",
    "- Change optimization algorithm\n",
    "- Measure training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation function\n",
    "- Add non-linearity into the network\n",
    "- Apply to a linear combination of its input\n",
    "- Various type of activation:\n",
    "  - Logistic sigmoid (`sigmoid`):\n",
    "  \n",
    "    \\begin{equation}\n",
    "    g(x) = \\frac{1}{1+e^{-x}}\n",
    "    \\end{equation}\n",
    "\n",
    "  - Hyperpolic tangent (`tanh`):\n",
    "  \n",
    "    \\begin{equation}\n",
    "    g(x) = tanh{x} = \\frac{e^x - e^{-x}}{e^x - e^{-x}}\n",
    "    \\end{equation}\n",
    "\n",
    "  - Rectified linear unit (`relu`):\n",
    "  \n",
    "    \\begin{equation}\n",
    "    g(x) = max(0,x)\n",
    "    \\end{equation}\n",
    "\n",
    "  - Softmax (`softmax`):\n",
    "\n",
    "    \\begin{equation}\n",
    "    g(\\mathbf{x})=\\frac{e^{x_i}}{\\sum_{j=1}^{D}}\n",
    "    \\end{equation}\n",
    "\n",
    "    - $D$ is the dimension of the vector $\\mathbf{x}$\n",
    "    - Usually, used with the output layer of a network to convert output to be probility value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10010     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 795,010\n",
      "Trainable params: 795,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.5250 - acc: 0.8571 - val_loss: 0.2973 - val_acc: 0.9161\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2911 - acc: 0.9156 - val_loss: 0.2580 - val_acc: 0.9282\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.2501 - acc: 0.9278 - val_loss: 0.2251 - val_acc: 0.9354\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.2161 - acc: 0.9366 - val_loss: 0.1979 - val_acc: 0.9442\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.1848 - acc: 0.9460 - val_loss: 0.1781 - val_acc: 0.9504\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.1576 - acc: 0.9549 - val_loss: 0.1617 - val_acc: 0.9541\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.1368 - acc: 0.9605 - val_loss: 0.1523 - val_acc: 0.9560\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.1188 - acc: 0.9656 - val_loss: 0.1298 - val_acc: 0.9636\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.1026 - acc: 0.9703 - val_loss: 0.1226 - val_acc: 0.9627\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0894 - acc: 0.9743 - val_loss: 0.1131 - val_acc: 0.9673\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0780 - acc: 0.9780 - val_loss: 0.1143 - val_acc: 0.9661\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0696 - acc: 0.9802 - val_loss: 0.0997 - val_acc: 0.9706\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0608 - acc: 0.9826 - val_loss: 0.0983 - val_acc: 0.9709\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0534 - acc: 0.9856 - val_loss: 0.0913 - val_acc: 0.9728\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0463 - acc: 0.9871 - val_loss: 0.0865 - val_acc: 0.9743\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0401 - acc: 0.9894 - val_loss: 0.0867 - val_acc: 0.9748\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0359 - acc: 0.9908 - val_loss: 0.0839 - val_acc: 0.9746\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0311 - acc: 0.9925 - val_loss: 0.0823 - val_acc: 0.9745\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0269 - acc: 0.9935 - val_loss: 0.0827 - val_acc: 0.9747\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0231 - acc: 0.9950 - val_loss: 0.0762 - val_acc: 0.9767\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0195 - acc: 0.9960 - val_loss: 0.0779 - val_acc: 0.9768\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0170 - acc: 0.9968 - val_loss: 0.0780 - val_acc: 0.9765\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0148 - acc: 0.9975 - val_loss: 0.0822 - val_acc: 0.9757\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0135 - acc: 0.9977 - val_loss: 0.0804 - val_acc: 0.9750\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.0109 - acc: 0.9986 - val_loss: 0.0748 - val_acc: 0.9768\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0093 - acc: 0.9989 - val_loss: 0.0766 - val_acc: 0.9778\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.0081 - acc: 0.9993 - val_loss: 0.0756 - val_acc: 0.9775\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.0069 - acc: 0.9994 - val_loss: 0.0756 - val_acc: 0.9784\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0058 - acc: 0.9995 - val_loss: 0.0778 - val_acc: 0.9781\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.0050 - acc: 0.9996 - val_loss: 0.0775 - val_acc: 0.9788\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0043 - acc: 0.9998 - val_loss: 0.0775 - val_acc: 0.9786\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0037 - acc: 0.9998 - val_loss: 0.0797 - val_acc: 0.9781\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0033 - acc: 0.9999 - val_loss: 0.0793 - val_acc: 0.9794\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0028 - acc: 0.9999 - val_loss: 0.0814 - val_acc: 0.9777\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.0026 - acc: 0.9999 - val_loss: 0.0793 - val_acc: 0.9783\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9779\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.0018 - acc: 0.9999 - val_loss: 0.0802 - val_acc: 0.9788\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.0017 - acc: 0.9999 - val_loss: 0.0828 - val_acc: 0.9784\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0813 - val_acc: 0.9790\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0826 - val_acc: 0.9785\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 9.6919e-04 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 0.9798\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 8.2721e-04 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9789\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 8.3932e-04 - acc: 1.0000 - val_loss: 0.0829 - val_acc: 0.9798\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 7.0116e-04 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 0.9798\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.0911 - val_acc: 0.9766\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.0858 - val_acc: 0.9794\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 5.0896e-04 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9801\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 4.1884e-04 - acc: 1.0000 - val_loss: 0.0866 - val_acc: 0.9798\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 3.6084e-04 - acc: 1.0000 - val_loss: 0.0862 - val_acc: 0.9802\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 3.2959e-04 - acc: 1.0000 - val_loss: 0.0868 - val_acc: 0.9806\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 2.9925e-04 - acc: 1.0000 - val_loss: 0.0869 - val_acc: 0.9804\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 2.7243e-04 - acc: 1.0000 - val_loss: 0.0877 - val_acc: 0.9804\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 2.5801e-04 - acc: 1.0000 - val_loss: 0.0882 - val_acc: 0.9798\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.2938e-04 - acc: 1.0000 - val_loss: 0.0890 - val_acc: 0.9802\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.3216e-04 - acc: 1.0000 - val_loss: 0.0886 - val_acc: 0.9804\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 1.9017e-04 - acc: 1.0000 - val_loss: 0.0888 - val_acc: 0.9808\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 1.7986e-04 - acc: 1.0000 - val_loss: 0.0901 - val_acc: 0.9805\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 1.6483e-04 - acc: 1.0000 - val_loss: 0.0903 - val_acc: 0.9813\n",
      "Epoch 59/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 1.4637e-04 - acc: 1.0000 - val_loss: 0.0923 - val_acc: 0.9797\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 1.4667e-04 - acc: 1.0000 - val_loss: 0.0913 - val_acc: 0.9804\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 1.3047e-04 - acc: 1.0000 - val_loss: 0.0915 - val_acc: 0.9807\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 1.2555e-04 - acc: 1.0000 - val_loss: 0.0944 - val_acc: 0.9800\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 1.1611e-04 - acc: 1.0000 - val_loss: 0.0926 - val_acc: 0.9808\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 9.0434e-05 - acc: 1.0000 - val_loss: 0.0933 - val_acc: 0.9812\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 8.1277e-05 - acc: 1.0000 - val_loss: 0.0942 - val_acc: 0.9812\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 7.1066e-05 - acc: 1.0000 - val_loss: 0.0940 - val_acc: 0.9808\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 6.6898e-05 - acc: 1.0000 - val_loss: 0.0955 - val_acc: 0.9808\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 6.1601e-05 - acc: 1.0000 - val_loss: 0.0961 - val_acc: 0.9811\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 6.4394e-05 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 0.9808\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 4.8465e-05 - acc: 1.0000 - val_loss: 0.0971 - val_acc: 0.9814\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 4.3009e-05 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9812\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 3.7366e-05 - acc: 1.0000 - val_loss: 0.0986 - val_acc: 0.9809\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 3.4016e-05 - acc: 1.0000 - val_loss: 0.0997 - val_acc: 0.9810\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 3.8867e-05 - acc: 1.0000 - val_loss: 0.1069 - val_acc: 0.9795\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0083 - acc: 0.9973 - val_loss: 0.1059 - val_acc: 0.9788\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 3.1723e-04 - acc: 1.0000 - val_loss: 0.0993 - val_acc: 0.9794\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 1.0643e-04 - acc: 1.0000 - val_loss: 0.0984 - val_acc: 0.9801\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 7.9467e-05 - acc: 1.0000 - val_loss: 0.0987 - val_acc: 0.9800\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 6.9243e-05 - acc: 1.0000 - val_loss: 0.0987 - val_acc: 0.9804\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 6.1107e-05 - acc: 1.0000 - val_loss: 0.0987 - val_acc: 0.9803\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 5.5320e-05 - acc: 1.0000 - val_loss: 0.0988 - val_acc: 0.9805\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 5.0341e-05 - acc: 1.0000 - val_loss: 0.0991 - val_acc: 0.9804\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 4.6174e-05 - acc: 1.0000 - val_loss: 0.0989 - val_acc: 0.9803\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 4.2487e-05 - acc: 1.0000 - val_loss: 0.0993 - val_acc: 0.9803\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 3.9163e-05 - acc: 1.0000 - val_loss: 0.0996 - val_acc: 0.9804\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 3.6345e-05 - acc: 1.0000 - val_loss: 0.0997 - val_acc: 0.9808\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 3.3789e-05 - acc: 1.0000 - val_loss: 0.1000 - val_acc: 0.9805\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 3.1756e-05 - acc: 1.0000 - val_loss: 0.1002 - val_acc: 0.9803\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.9488e-05 - acc: 1.0000 - val_loss: 0.1004 - val_acc: 0.9807\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.7539e-05 - acc: 1.0000 - val_loss: 0.1006 - val_acc: 0.9811\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.5725e-05 - acc: 1.0000 - val_loss: 0.1010 - val_acc: 0.9805\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.4109e-05 - acc: 1.0000 - val_loss: 0.1011 - val_acc: 0.9811\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.2701e-05 - acc: 1.0000 - val_loss: 0.1013 - val_acc: 0.9808\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.1272e-05 - acc: 1.0000 - val_loss: 0.1017 - val_acc: 0.9810\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 1.9955e-05 - acc: 1.0000 - val_loss: 0.1021 - val_acc: 0.9811\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 1.8687e-05 - acc: 1.0000 - val_loss: 0.1022 - val_acc: 0.9816\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 1.7582e-05 - acc: 1.0000 - val_loss: 0.1023 - val_acc: 0.9811\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 1.6696e-05 - acc: 1.0000 - val_loss: 0.1028 - val_acc: 0.9813\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 1.5554e-05 - acc: 1.0000 - val_loss: 0.1026 - val_acc: 0.9813\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 1.4607e-05 - acc: 1.0000 - val_loss: 0.1033 - val_acc: 0.9815\n",
      "Time =  276.15770292282104\n",
      "10000/10000 [==============================] - 1s 73us/step\n",
      "Test score: 0.09786079965312129\n",
      "Test accuracy: 0.9822\n"
     ]
    }
   ],
   "source": [
    "#import necessary modules\n",
    "from keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "import time\n",
    "\n",
    "#network structure\n",
    "INPUT_DIM = 784\n",
    "NUM_HIDDEN = 1000\n",
    "NUM_CLASS = 10\n",
    "\n",
    "#loading dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, INPUT_DIM).astype('float32')\n",
    "X_test = X_test.reshape(10000, INPUT_DIM).astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "#building a model\n",
    "model = Sequential()\n",
    "model.add(Dense(NUM_HIDDEN, input_shape=(INPUT_DIM,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(NUM_CLASS))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#train the model\n",
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size=200, epochs=100, verbose=1, validation_split=0.2)\n",
    "print(\"Time = \", time.time() - start_time)\n",
    "\n",
    "#evaluate the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analyze training performance\n",
    "- Plot accuracy and loss against epochs\n",
    "- Use a package named `matplotlib`\n",
    "  - Support various types of graph\n",
    "  - <https://matplotlib.org/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['training set', 'validation set'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "- Dropout is a technique used to regularize deep neural networks.\n",
    "  - Regularization means any modification aiming at improving the generalization performance, i.e,. test accuracy.\n",
    "    -Training accuracy may or may not be improved.\n",
    "- Dropout randomly _drops_ (or tempolarity removes) some neurons from the calculation.\n",
    "  - The remaining neurons are forced to learn some useful features on their own.\n",
    "    - This can avoid the case that the network is being dominated by some neurons.\n",
    "  - A paramater controlling the number of neurons that will be dropped is known as _drop rate_.\n",
    "  \n",
    "- Ref. N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: A simple way to prevent neural networks from overfitting,\" Journal of Machine Learning Research, vol.15, pp.1929--1958, 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                10010     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 795,010\n",
      "Trainable params: 795,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.6173 - acc: 0.8096 - val_loss: 0.3057 - val_acc: 0.9132\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.3369 - acc: 0.8995 - val_loss: 0.2608 - val_acc: 0.9264\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.2860 - acc: 0.9165 - val_loss: 0.2325 - val_acc: 0.9338\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.2440 - acc: 0.9275 - val_loss: 0.2019 - val_acc: 0.9435\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.2123 - acc: 0.9376 - val_loss: 0.1738 - val_acc: 0.9510\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1851 - acc: 0.9467 - val_loss: 0.1568 - val_acc: 0.9560\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.1636 - acc: 0.9515 - val_loss: 0.1440 - val_acc: 0.9591\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.1456 - acc: 0.9565 - val_loss: 0.1335 - val_acc: 0.9618\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.1309 - acc: 0.9609 - val_loss: 0.1211 - val_acc: 0.9654\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1199 - acc: 0.9645 - val_loss: 0.1168 - val_acc: 0.9650\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1089 - acc: 0.9680 - val_loss: 0.1078 - val_acc: 0.9673\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.1002 - acc: 0.9699 - val_loss: 0.1057 - val_acc: 0.9693\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0913 - acc: 0.9725 - val_loss: 0.0959 - val_acc: 0.9713\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0820 - acc: 0.9753 - val_loss: 0.0920 - val_acc: 0.9728\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0781 - acc: 0.9765 - val_loss: 0.0873 - val_acc: 0.9743\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0727 - acc: 0.9774 - val_loss: 0.0872 - val_acc: 0.9741\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0682 - acc: 0.9795 - val_loss: 0.0844 - val_acc: 0.9745\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0619 - acc: 0.9811 - val_loss: 0.0810 - val_acc: 0.9748\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0591 - acc: 0.9818 - val_loss: 0.0780 - val_acc: 0.9769\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0546 - acc: 0.9829 - val_loss: 0.0787 - val_acc: 0.9771\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0505 - acc: 0.9842 - val_loss: 0.0756 - val_acc: 0.9762\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0480 - acc: 0.9853 - val_loss: 0.0740 - val_acc: 0.9771\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0433 - acc: 0.9867 - val_loss: 0.0763 - val_acc: 0.9773\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0410 - acc: 0.9875 - val_loss: 0.0734 - val_acc: 0.9779\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0388 - acc: 0.9881 - val_loss: 0.0729 - val_acc: 0.9780\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0371 - acc: 0.9888 - val_loss: 0.0735 - val_acc: 0.9778\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0340 - acc: 0.9897 - val_loss: 0.0725 - val_acc: 0.9794\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0331 - acc: 0.9902 - val_loss: 0.0688 - val_acc: 0.9793\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0294 - acc: 0.9915 - val_loss: 0.0681 - val_acc: 0.9800\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0289 - acc: 0.9908 - val_loss: 0.0715 - val_acc: 0.9786\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0272 - acc: 0.9913 - val_loss: 0.0704 - val_acc: 0.9787\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0261 - acc: 0.9921 - val_loss: 0.0688 - val_acc: 0.9808\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0236 - acc: 0.9930 - val_loss: 0.0687 - val_acc: 0.9797\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0230 - acc: 0.9933 - val_loss: 0.0689 - val_acc: 0.9800\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0217 - acc: 0.9932 - val_loss: 0.0681 - val_acc: 0.9807\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0673 - val_acc: 0.9818\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0191 - acc: 0.9945 - val_loss: 0.0686 - val_acc: 0.9814\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0195 - acc: 0.9943 - val_loss: 0.0695 - val_acc: 0.9811\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0174 - acc: 0.9947 - val_loss: 0.0726 - val_acc: 0.9795\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.0720 - val_acc: 0.9809\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0166 - acc: 0.9951 - val_loss: 0.0689 - val_acc: 0.9808\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0160 - acc: 0.9951 - val_loss: 0.0687 - val_acc: 0.9810\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0147 - acc: 0.9955 - val_loss: 0.0684 - val_acc: 0.9815\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0147 - acc: 0.9956 - val_loss: 0.0684 - val_acc: 0.9816\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0128 - acc: 0.9961 - val_loss: 0.0676 - val_acc: 0.9815\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0124 - acc: 0.9961 - val_loss: 0.0683 - val_acc: 0.9828\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0125 - acc: 0.9964 - val_loss: 0.0709 - val_acc: 0.9811\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0124 - acc: 0.9964 - val_loss: 0.0722 - val_acc: 0.9813\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0708 - val_acc: 0.9812\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0108 - acc: 0.9971 - val_loss: 0.0733 - val_acc: 0.9820\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0104 - acc: 0.9969 - val_loss: 0.0720 - val_acc: 0.9816\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0106 - acc: 0.9969 - val_loss: 0.0711 - val_acc: 0.9820\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0095 - acc: 0.9972 - val_loss: 0.0735 - val_acc: 0.9827\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0726 - val_acc: 0.9829\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0093 - acc: 0.9971 - val_loss: 0.0712 - val_acc: 0.9829\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0092 - acc: 0.9974 - val_loss: 0.0740 - val_acc: 0.9823\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0739 - val_acc: 0.9827\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0735 - val_acc: 0.9825\n",
      "Epoch 59/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.0743 - val_acc: 0.9834\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0072 - acc: 0.9980 - val_loss: 0.0728 - val_acc: 0.9829\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0723 - val_acc: 0.9828\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0076 - acc: 0.9978 - val_loss: 0.0705 - val_acc: 0.9832\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0071 - acc: 0.9982 - val_loss: 0.0729 - val_acc: 0.9827\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0067 - acc: 0.9981 - val_loss: 0.0785 - val_acc: 0.9817\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0761 - val_acc: 0.9829\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0754 - val_acc: 0.9829\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0068 - acc: 0.9981 - val_loss: 0.0750 - val_acc: 0.9835\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0054 - acc: 0.9985 - val_loss: 0.0764 - val_acc: 0.9833\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0057 - acc: 0.9986 - val_loss: 0.0762 - val_acc: 0.9826\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0770 - val_acc: 0.9831\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0758 - val_acc: 0.9830\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0058 - acc: 0.9984 - val_loss: 0.0746 - val_acc: 0.9833\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.0756 - val_acc: 0.9838\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0792 - val_acc: 0.9824\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0760 - val_acc: 0.9832\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0051 - acc: 0.9987 - val_loss: 0.0774 - val_acc: 0.9834\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0766 - val_acc: 0.9833\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0788 - val_acc: 0.9841\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.0048 - acc: 0.9984 - val_loss: 0.0797 - val_acc: 0.9825\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0790 - val_acc: 0.9835\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0758 - val_acc: 0.9828\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0046 - acc: 0.9989 - val_loss: 0.0804 - val_acc: 0.9828\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0789 - val_acc: 0.9836\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0802 - val_acc: 0.9831\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.0805 - val_acc: 0.9833\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0795 - val_acc: 0.9828\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0809 - val_acc: 0.9839\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0815 - val_acc: 0.9835\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0834 - val_acc: 0.9824\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0807 - val_acc: 0.9827\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0809 - val_acc: 0.9831\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0811 - val_acc: 0.9836\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0802 - val_acc: 0.9838\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0831 - val_acc: 0.9832\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0827 - val_acc: 0.9842\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0831 - val_acc: 0.9842\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0852 - val_acc: 0.9833\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0853 - val_acc: 0.9828\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0828 - val_acc: 0.9830\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0859 - val_acc: 0.9829\n",
      "Time =  288.70132327079773\n",
      "10000/10000 [==============================] - 1s 76us/step\n",
      "Test score: 0.08589731342030155\n",
      "Test accuracy: 0.9832\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "#building a model\n",
    "model = Sequential()\n",
    "model.add(Dense(NUM_HIDDEN, input_shape=(INPUT_DIM,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASS))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#train the model\n",
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size=200, epochs=100, verbose=1, validation_split=0.2)\n",
    "print(\"Time = \", time.time() - start_time)\n",
    "\n",
    "#evaluate the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional neural network\n",
    "\n",
    "- A __convolutional neural network (CNN)__ is a class of feed-forward neural networks mostly applied to solve computer vision problems.\n",
    "- A CNN usually consists of various types of layers:\n",
    "  - Convolutional layer\n",
    "  - Pooling layer\n",
    "  - Fully-connected layer\n",
    "- Convolutional layers make CNNs distinct from other classes of neural networks.\n",
    "  - Use the concept of weight-sharing.\n",
    "  \n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png>\n",
    "\n",
    "Image from: https://en.wikipedia.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution layers\n",
    "\n",
    "- A convolution layer contains a number of filters, which are applied to an input of the layer by using convolution.\n",
    "  - Input: a $C$-channel image (volume) of size $M \\times N$\n",
    "  - Output: a $X$-channel image of size $M \\times N$ (or smaller)\n",
    "    - $X$ is the number of filters in the convolution layer\n",
    "    - One filter produces one channel in the output image.\n",
    "    - Each filter usually has single channel.\n",
    "- Each filter is adjusted during training to extract a local feature useful for the problem at hand.\n",
    "  - Require no hand-crafted features\n",
    "  - But automatically learn useful features from data\n",
    "- Usually, a convolution layer in the beginning of the network extracts low-level features.\n",
    "- A subsequent convolution layer can extract relatively more complicated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling layers\n",
    "\n",
    "- A pooling layer down-samples an input image by using maximum function.\n",
    "  - Apply a max filter of size $S \\times S$ to each channel in the input image.\n",
    "  - Usually a stride of the size S is also used.\n",
    "\n",
    "- Two main purposes of a pooling layer:\n",
    "  - To make CNN becomes more invariant to a small translation\n",
    "  - To reduce the amount of data feeding to the subsequent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fully-connected layers\n",
    "\n",
    "- Each node in a fully-connected layer has a connection to each node in the previous layer.\n",
    "  - Also known as a dense layer\n",
    "- Each connection has a numerical value, called weight, associated with it.\n",
    "  - A weight determines the significance of the connected input.\n",
    "- The output of a node 𝑖 in fully-connected is described by\n",
    "\n",
    "  \\begin{equation}\n",
    "  h_i = g(w_{i1}x_1 + w_{i2}x_2 + \\cdots + w_{iD}x_D + b_i)\n",
    "  \\end{equation}\n",
    "\n",
    "  - $w_{ij}$ is the weight of connection between the node $𝑖$ and input $x_j$.\n",
    "  - $D$ is the size of the input vector (the number of nodes in the previous layer).\n",
    "  - $b_i$ is the bias of the node $i$.\n",
    "  - $g(\\cdot)$ is an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
    "\n",
    "#parameters\n",
    "INPUT_ROWS = 10\n",
    "INPUT_COLS = 10\n",
    "INPUT_CHS  = 1\n",
    "NUM_HIDDEN = 1000\n",
    "NUM_CLASS  = 10\n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "#(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "#(X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "X_train = X_train.reshape(X_train.shape[0], INPUT_ROWS, INPUT_COLS, INPUT_CHS).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], INPUT_ROWS, INPUT_COLS, INPUT_CHS).astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              3137000   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                10010     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 3,165,826\n",
      "Trainable params: 3,165,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#import necessary modules\n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "#building a CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(INPUT_ROWS, INPUT_COLS, INPUT_CHS,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None)) \n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None))    \n",
    "model.add(Flatten())   \n",
    "model.add(Dense(NUM_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NUM_CLASS))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "48000/48000 [==============================] - 17s 348us/step - loss: 0.2197 - acc: 0.9337 - val_loss: 0.0623 - val_acc: 0.9816\n",
      "Epoch 2/30\n",
      "48000/48000 [==============================] - 13s 275us/step - loss: 0.0525 - acc: 0.9835 - val_loss: 0.0496 - val_acc: 0.9843\n",
      "Epoch 3/30\n",
      "48000/48000 [==============================] - 13s 275us/step - loss: 0.0363 - acc: 0.9890 - val_loss: 0.0383 - val_acc: 0.9882\n",
      "Epoch 4/30\n",
      "48000/48000 [==============================] - 13s 274us/step - loss: 0.0250 - acc: 0.9923 - val_loss: 0.0389 - val_acc: 0.9879\n",
      "Epoch 5/30\n",
      "48000/48000 [==============================] - 13s 273us/step - loss: 0.0186 - acc: 0.9934 - val_loss: 0.0395 - val_acc: 0.9887\n",
      "Epoch 6/30\n",
      "48000/48000 [==============================] - 13s 273us/step - loss: 0.0137 - acc: 0.9956 - val_loss: 0.0348 - val_acc: 0.9908\n",
      "Epoch 7/30\n",
      "48000/48000 [==============================] - 13s 273us/step - loss: 0.0124 - acc: 0.9960 - val_loss: 0.0410 - val_acc: 0.9892\n",
      "Epoch 8/30\n",
      "48000/48000 [==============================] - 13s 276us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0407 - val_acc: 0.9900\n",
      "Epoch 9/30\n",
      "48000/48000 [==============================] - 13s 274us/step - loss: 0.0066 - acc: 0.9979 - val_loss: 0.0409 - val_acc: 0.9895\n",
      "Epoch 10/30\n",
      "48000/48000 [==============================] - 13s 272us/step - loss: 0.0059 - acc: 0.9978 - val_loss: 0.0370 - val_acc: 0.9908\n",
      "Epoch 11/30\n",
      "48000/48000 [==============================] - 13s 273us/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.0427 - val_acc: 0.9880\n",
      "Epoch 12/30\n",
      "48000/48000 [==============================] - 13s 274us/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0374 - val_acc: 0.9918\n",
      "Epoch 13/30\n",
      "48000/48000 [==============================] - 13s 274us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0480 - val_acc: 0.9890\n",
      "Epoch 14/30\n",
      "48000/48000 [==============================] - 13s 274us/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0370 - val_acc: 0.9927\n",
      "Epoch 15/30\n",
      "48000/48000 [==============================] - 13s 272us/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0407 - val_acc: 0.9908\n",
      "Epoch 16/30\n",
      "48000/48000 [==============================] - 13s 271us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0504 - val_acc: 0.9888\n",
      "Epoch 17/30\n",
      "48000/48000 [==============================] - 13s 272us/step - loss: 0.0040 - acc: 0.9985 - val_loss: 0.0421 - val_acc: 0.9920\n",
      "Epoch 18/30\n",
      "48000/48000 [==============================] - 13s 275us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0510 - val_acc: 0.9891\n",
      "Epoch 19/30\n",
      "48000/48000 [==============================] - 13s 275us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0439 - val_acc: 0.9912\n",
      "Epoch 20/30\n",
      "48000/48000 [==============================] - 13s 272us/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0424 - val_acc: 0.9909\n",
      "Epoch 21/30\n",
      "48000/48000 [==============================] - 13s 272us/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0393 - val_acc: 0.9921\n",
      "Epoch 22/30\n",
      "48000/48000 [==============================] - 13s 273us/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0466 - val_acc: 0.9909\n",
      "Epoch 23/30\n",
      "48000/48000 [==============================] - 13s 270us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0433 - val_acc: 0.9908\n",
      "Epoch 24/30\n",
      "48000/48000 [==============================] - 13s 270us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0420 - val_acc: 0.9913\n",
      "Epoch 25/30\n",
      "48000/48000 [==============================] - 13s 271us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0490 - val_acc: 0.9901\n",
      "Epoch 26/30\n",
      "48000/48000 [==============================] - 13s 270us/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0458 - val_acc: 0.9918\n",
      "Epoch 27/30\n",
      "48000/48000 [==============================] - 13s 271us/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0493 - val_acc: 0.9901\n",
      "Epoch 28/30\n",
      "48000/48000 [==============================] - 13s 270us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0392 - val_acc: 0.9930\n",
      "Epoch 29/30\n",
      "48000/48000 [==============================] - 13s 269us/step - loss: 1.3066e-04 - acc: 1.0000 - val_loss: 0.0388 - val_acc: 0.9935\n",
      "Epoch 30/30\n",
      "48000/48000 [==============================] - 13s 271us/step - loss: 5.6882e-05 - acc: 1.0000 - val_loss: 0.0385 - val_acc: 0.9939\n",
      "Time =  397.0586175918579\n",
      "10000/10000 [==============================] - 2s 174us/step\n",
      "Test score: 0.03383443017396969\n",
      "Test accuracy: 0.9932\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size=200, epochs=30, verbose=1, validation_split=0.2)\n",
    "print(\"Time = \", time.time() - start_time)\n",
    "\n",
    "#evaluate the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "- Need to change the input of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                10010     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,126,402\n",
      "Trainable params: 4,126,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "40000/40000 [==============================] - 15s 364us/step - loss: 1.5679 - acc: 0.4388 - val_loss: 1.3052 - val_acc: 0.5339\n",
      "Epoch 2/30\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 1.1505 - acc: 0.5938 - val_loss: 1.1063 - val_acc: 0.6091\n",
      "Epoch 3/30\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.9733 - acc: 0.6575 - val_loss: 0.9854 - val_acc: 0.6555\n",
      "Epoch 4/30\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.8540 - acc: 0.7005 - val_loss: 0.9365 - val_acc: 0.6758\n",
      "Epoch 5/30\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.7517 - acc: 0.7382 - val_loss: 0.8956 - val_acc: 0.6942\n",
      "Epoch 6/30\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.6590 - acc: 0.7708 - val_loss: 0.8907 - val_acc: 0.6999\n",
      "Epoch 7/30\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.5629 - acc: 0.8048 - val_loss: 0.8762 - val_acc: 0.7041\n",
      "Epoch 8/30\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.4707 - acc: 0.8400 - val_loss: 0.8795 - val_acc: 0.7200\n",
      "Epoch 9/30\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.3745 - acc: 0.8756 - val_loss: 0.8863 - val_acc: 0.7185\n",
      "Epoch 10/30\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.2891 - acc: 0.9076 - val_loss: 0.9594 - val_acc: 0.7216\n",
      "Epoch 11/30\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.2143 - acc: 0.9338 - val_loss: 1.0671 - val_acc: 0.7081\n",
      "Epoch 12/30\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.1480 - acc: 0.9575 - val_loss: 1.0788 - val_acc: 0.7217\n",
      "Epoch 13/30\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.0972 - acc: 0.9753 - val_loss: 1.1981 - val_acc: 0.7180\n",
      "Epoch 14/30\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.0657 - acc: 0.9850 - val_loss: 1.2735 - val_acc: 0.7236\n",
      "Epoch 15/30\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0410 - acc: 0.9922 - val_loss: 1.3469 - val_acc: 0.7221\n",
      "Epoch 16/30\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.0417 - acc: 0.9912 - val_loss: 1.4267 - val_acc: 0.7077\n",
      "Epoch 17/30\n",
      "40000/40000 [==============================] - 15s 384us/step - loss: 0.0430 - acc: 0.9896 - val_loss: 1.5542 - val_acc: 0.7043\n",
      "Epoch 18/30\n",
      "40000/40000 [==============================] - 15s 383us/step - loss: 0.0224 - acc: 0.9960 - val_loss: 1.5408 - val_acc: 0.7192\n",
      "Epoch 19/30\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.0227 - acc: 0.9957 - val_loss: 1.5784 - val_acc: 0.7178\n",
      "Epoch 20/30\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.0311 - acc: 0.9923 - val_loss: 1.5538 - val_acc: 0.7138\n",
      "Epoch 21/30\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.0411 - acc: 0.9874 - val_loss: 1.6767 - val_acc: 0.6977\n",
      "Epoch 22/30\n",
      "40000/40000 [==============================] - 15s 382us/step - loss: 0.0647 - acc: 0.9784 - val_loss: 1.7170 - val_acc: 0.6979\n",
      "Epoch 23/30\n",
      "40000/40000 [==============================] - 15s 384us/step - loss: 0.0373 - acc: 0.9893 - val_loss: 1.6970 - val_acc: 0.7129\n",
      "Epoch 24/30\n",
      "40000/40000 [==============================] - 15s 384us/step - loss: 0.0216 - acc: 0.9944 - val_loss: 1.7441 - val_acc: 0.7114\n",
      "Epoch 25/30\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.0255 - acc: 0.9931 - val_loss: 1.7492 - val_acc: 0.7177\n",
      "Epoch 26/30\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.0205 - acc: 0.9941 - val_loss: 1.7780 - val_acc: 0.7148\n",
      "Epoch 27/30\n",
      "40000/40000 [==============================] - 15s 385us/step - loss: 0.0173 - acc: 0.9955 - val_loss: 1.8200 - val_acc: 0.7155\n",
      "Epoch 28/30\n",
      "40000/40000 [==============================] - 15s 385us/step - loss: 0.0057 - acc: 0.9991 - val_loss: 1.8663 - val_acc: 0.7198\n",
      "Epoch 29/30\n",
      "40000/40000 [==============================] - 15s 386us/step - loss: 0.0361 - acc: 0.9888 - val_loss: 1.8394 - val_acc: 0.6949\n",
      "Epoch 30/30\n",
      "40000/40000 [==============================] - 15s 382us/step - loss: 0.0518 - acc: 0.9833 - val_loss: 1.8422 - val_acc: 0.7066\n",
      "Time =  454.1632478237152\n",
      "10000/10000 [==============================] - 2s 211us/step\n",
      "Test score: 1.8918208993911743\n",
      "Test accuracy: 0.6987\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
    "\n",
    "#parameters\n",
    "INPUT_ROWS = 32\n",
    "INPUT_COLS = 32\n",
    "INPUT_CHS  = 3\n",
    "NUM_HIDDEN = 1000\n",
    "NUM_CLASS  = 10\n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], INPUT_ROWS, INPUT_ROWS, INPUT_CHS).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], INPUT_ROWS, INPUT_ROWS, INPUT_CHS).astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "#build a CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(INPUT_ROWS,INPUT_COLS,INPUT_CHS,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None)) \n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None))    \n",
    "model.add(Flatten())   \n",
    "model.add(Dense(NUM_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NUM_CLASS))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#train the model\n",
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size=200, epochs=30, verbose=1, validation_split=0.2)\n",
    "print(\"Time = \", time.time() - start_time)\n",
    "\n",
    "#evaluate the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Existing deep neural network models\n",
    "\n",
    "- Keras allows us to load a pretrained existing deep neural network model  \n",
    "  - For example, VGG16, VGG19, ResNet50, InceptionV3\n",
    "  - `import keras.applications`\n",
    "  \n",
    "## Transfer learning\n",
    "- Transfer learning aims to apply knowledge obtained from solvging one problem $A$ to solve another problem $B$.\n",
    "  - Need no to start from scratch\n",
    "  - Reduce training time\n",
    "- A simple approach\n",
    "  - Borrow some early layers of an existing network trained for solving the problem $A$\n",
    "  - Append new layer(s) to the borrowed layers\n",
    "  - Train the network for solving the problem $B$\n",
    "    - The borrowed network may be freezed (remain unchanged) or may be re-trained (fine-tuned)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Pokemon classification\n",
    "- **Credits:** source code and data by Boonnithi Jiramaneepinit\n",
    "- **Objective:** To classify the type of Pokemon\n",
    "  - Five classes: Pikachu, Bulbasaur (Fushigidane), Squirtle (Zenigame), Charmander (Hitokage), Mewtwo    \n",
    "    <img src=\"figures/Pokemon.bmp\" width=\"100%\">\n",
    "    Images from: https://en.wikipedia.org\n",
    "    \n",
    "- **Training dataset:**\n",
    "  - Input: Color images\n",
    "  - Size: Varying\n",
    "  - Background: Plain or complex background\n",
    "  - Number of images:\n",
    "    - Pikachu: 234 images\n",
    "    - Bulbasaur: 234 images \n",
    "    - Squirtle: 223 images\n",
    "    - Charmander: 238 images\n",
    "    - Mewtwo: 239 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading training dataset from Moodle\n",
    "\n",
    "- Create a folder named 'dataset' in a working directory of Python interpreter\n",
    "- Download from Moodle all zip files to the folder\n",
    "- Unzip all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] data matrix: 273.52MB\n",
      "[INFO] compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukrit\\Anaconda3\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000001DAFCBB9C88> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000001DB35936F28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DB35936CF8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DB3593D160> False\n",
      "<keras.layers.core.Activation object at 0x000001DB3593D208> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000001DA98F00E80> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001DB3593DDD8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAFCBECD30> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DA9AF2F828> False\n",
      "<keras.layers.core.Activation object at 0x000001DA9AF2F9E8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DA9AF7DE10> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DA9AF69E10> False\n",
      "<keras.layers.core.Activation object at 0x000001DA9AFC5080> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DA9B035AC8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DA9B10CE80> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DA9B052630> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DA9B198D30> False\n",
      "<keras.layers.merge.Add object at 0x000001DA9B15F9B0> False\n",
      "<keras.layers.core.Activation object at 0x000001DA9B1FEBE0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DA9B1FEB38> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DA9B259668> False\n",
      "<keras.layers.core.Activation object at 0x000001DA9B2AFBA8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA577C780> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA5795438> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA57D2978> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA5852C88> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA58863C8> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA58CB400> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA5942BE0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA5942710> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA599E6A0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA59F5BE0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA5A3C7B8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA5A5A470> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA5A961D0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA5B15CC0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA5B477B8> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA5B8C438> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA5C03C18> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA5C03748> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA5C60198> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA5CB6C18> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA5D037F0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA5D1A4A8> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA5D5B208> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA5DD6CF8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA5E4E470> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA5E0B7F0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA5EE8EF0> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA5F9F978> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA5FC5358> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA5FC5518> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA60212E8> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA6071B00> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA6099F60> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA60B7EF0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA611A278> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA618EF60> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA61B1F98> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA6261F60> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA6287390> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA6287550> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA62E4048> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA6335B38> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA635EA20> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA6393B00> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA63DD2B0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA6456F98> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA6471FD0> False\n",
      "<keras.layers.merge.Add object at 0x000001DB32CD8BA8> False\n",
      "<keras.layers.core.Activation object at 0x000001DB33649470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DB336497F0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA658D0B8> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA65D8F98> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA6620588> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA6636240> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA66758D0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA66EDBA8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA67226D8> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA67642B0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA67DBC18> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA67DB748> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA683A898> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA688FCF8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA68D4A58> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA68F15C0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA69A2F28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA69C8470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA6A23630> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA69E3128> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA6B1B160> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA6AEAB70> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA6B91C50> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA6B918D0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA6BE93C8> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA6C59978> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA6CB8EB8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA6CA0D68> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA6D00160> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA6D78B38> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA6D95FD0> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA6DD1668> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA6E6F588> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA6E6F5F8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA6EC6898> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA6F18A90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA6F606A0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA6F7E358> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA6FB9860> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA7036CC0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA70687F0> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA70AC2E8> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA7127828> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA7127860> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA71809B0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA71F22B0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA7252E80> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA7239D68> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA729A1D0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA730DB38> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA732C6A0> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA7369BE0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA7406588> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA74065F8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA745F898> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA74AEA90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA74FC6A0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA7515358> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA754F860> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA75D0CC0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA76037F0> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA76442E8> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA76BC828> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA76BC860> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA77169B0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA77882B0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA77EBE80> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA77CFD68> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA78311D0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA78A5B38> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA78C46A0> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA7900BE0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA799A588> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA799A5F8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA79F3898> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA7A47A90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA7A926A0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA7AAC358> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA7AE8860> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA7B6ACC0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA7BDC2E8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA7B987F0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA7C749B0> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA7CADF28> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA7D44B00> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA7D44B70> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA7DA56A0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA7E1B668> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA7E40B00> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA7E5A978> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA7E99898> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA7F32780> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA7F51438> False\n",
      "<keras.layers.merge.Add object at 0x000001DAA7F8D2B0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA80292E8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA80291D0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA80824A8> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA80FFEF0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA811E438> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA81350F0> False\n",
      "<keras.layers.core.Activation object at 0x000001DAA81765F8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001DAA8224DA0> True\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001DAA820BE48> True\n",
      "<keras.layers.merge.Add object at 0x000001DAA826A2E8> True\n",
      "<keras.layers.core.Activation object at 0x000001DAA82E4B70> True\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 4, 4, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               8388864   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 31,977,861\n",
      "Trainable params: 9,444,869\n",
      "Non-trainable params: 22,532,992\n",
      "_________________________________________________________________\n",
      "[INFO] training network...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 933 samples, validate on 234 samples\n",
      "Epoch 1/20\n",
      "933/933 [==============================] - 17s 19ms/step - loss: 1.6582 - acc: 0.6806 - val_loss: 0.4545 - val_acc: 0.8761\n",
      "Epoch 2/20\n",
      "933/933 [==============================] - 9s 10ms/step - loss: 0.3029 - acc: 0.9003 - val_loss: 0.5114 - val_acc: 0.8803\n",
      "Epoch 3/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1818 - acc: 0.9432 - val_loss: 0.6825 - val_acc: 0.8761\n",
      "Epoch 4/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1321 - acc: 0.9646 - val_loss: 0.5942 - val_acc: 0.9103\n",
      "Epoch 5/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1682 - acc: 0.9636 - val_loss: 0.7882 - val_acc: 0.8846\n",
      "Epoch 6/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1150 - acc: 0.9839 - val_loss: 0.8630 - val_acc: 0.9017\n",
      "Epoch 7/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.0975 - acc: 0.9775 - val_loss: 1.2652 - val_acc: 0.8632\n",
      "Epoch 8/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.0901 - acc: 0.9818 - val_loss: 0.9612 - val_acc: 0.9188\n",
      "Epoch 9/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1557 - acc: 0.9732 - val_loss: 1.0305 - val_acc: 0.9103\n",
      "Epoch 10/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1209 - acc: 0.9807 - val_loss: 0.9979 - val_acc: 0.8889\n",
      "Epoch 11/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1126 - acc: 0.9861 - val_loss: 1.0693 - val_acc: 0.9060\n",
      "Epoch 12/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.0777 - acc: 0.9850 - val_loss: 1.0974 - val_acc: 0.8718\n",
      "Epoch 13/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1184 - acc: 0.9861 - val_loss: 0.9382 - val_acc: 0.8974\n",
      "Epoch 14/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1142 - acc: 0.9882 - val_loss: 1.0072 - val_acc: 0.9060\n",
      "Epoch 15/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.0454 - acc: 0.9936 - val_loss: 1.0602 - val_acc: 0.9060\n",
      "Epoch 16/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1264 - acc: 0.9882 - val_loss: 1.1451 - val_acc: 0.8974\n",
      "Epoch 17/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.0917 - acc: 0.9914 - val_loss: 1.1576 - val_acc: 0.9017\n",
      "Epoch 18/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1117 - acc: 0.9871 - val_loss: 1.1659 - val_acc: 0.8932\n",
      "Epoch 19/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.0707 - acc: 0.9946 - val_loss: 1.1130 - val_acc: 0.9060\n",
      "Epoch 20/20\n",
      "933/933 [==============================] - 10s 11ms/step - loss: 0.1149 - acc: 0.9850 - val_loss: 1.1224 - val_acc: 0.9103\n",
      "[INFO] serializing network...\n",
      "[INFO] serializing label binarizer...\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python train.py --dataset dataset --model pokedex.model --labelbin lb.pickle\n",
    "\n",
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os, time\n",
    "\n",
    "args = dict()\n",
    "args[\"dataset\"] = \"dataset\"\n",
    "args[\"model\"] = \"trained_model\" + str(time.time())\n",
    "args[\"labelbin\"] = \"label_bin\"\n",
    "args[\"plot\"] = \"plot.png\"\n",
    "\n",
    "# initialize the number of epochs to train for, initial learning rate,\n",
    "# batch size, and image dimensions\n",
    "EPOCHS = 20\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "IMAGE_DIMS = (100, 100, 3)\n",
    "\n",
    "# initialize the data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# grab the image paths and randomly shuffle them\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = sorted(list(paths.list_images(args[\"dataset\"])))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "\t# load the image, pre-process it, and store it in the data list\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "\timage = img_to_array(image)\n",
    "\tdata.append(image)\n",
    " \n",
    "\t# extract the class label from the image path and update the\n",
    "\t# labels list\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\tlabels.append(label)\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "print(\"[INFO] data matrix: {:.2f}MB\".format(\n",
    "\tdata.nbytes / (1024 * 1000.0)))\n",
    "\n",
    "# binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# initialize the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "vgg_conv = ResNet50(weights='imagenet',\n",
    "                    include_top=False,\n",
    "                    input_shape=(IMAGE_DIMS[1], IMAGE_DIMS[0], IMAGE_DIMS[2]))\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(trainY.shape[1], activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "print(\"[INFO] compiling model done\")\n",
    "\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(trainX,\n",
    "              trainY,\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=BS,\n",
    "              validation_data=(testX, testY))\n",
    "print(\"[INFO] training network done\")\n",
    "\n",
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(args[\"model\"])\n",
    "print(\"[INFO] serializing network done\")\n",
    "\n",
    "# save the label binarizer to disk\n",
    "print(\"[INFO] serializing label binarizer...\")\n",
    "f = open(args[\"labelbin\"], \"wb\")\n",
    "f.write(pickle.dumps(lb))\n",
    "f.close()\n",
    "print(\"[INFO] serializing label binarizer done\")\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = EPOCHS\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "- Download a test dataset from Moodle\n",
    "- Unzip and place it in a working directory of Python interpreter\n",
    "- You may also download a pretrained model from Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network...\n",
      "[INFO] classifying image...\n",
      "[INFO] bulbasaur: 99.74% \n",
      "[INFO] classifying image...\n",
      "[INFO] bulbasaur: 99.96% \n",
      "[INFO] classifying image...\n",
      "[INFO] bulbasaur: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] bulbasaur: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] charmander: 97.73% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] squirtle: 80.59% \n",
      "[INFO] classifying image...\n",
      "[INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] squirtle: 99.99% \n",
      "[INFO] classifying image...\n",
      "[INFO] squirtle: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 87.34% \n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python classify.py --model pokedex.model --labelbin lb.pickle --image examples/charmander_counter.png\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "args = dict()\n",
    "args[\"imagefolder\"] = \"proc_examples/\"\n",
    "args[\"model\"] = \"trained_model1542024603.305989\"  #need to change the name of the model\n",
    "args[\"labelbin\"] = \"label_bin\"\n",
    "args[\"plot\"] = \"plot.png\"\n",
    "\n",
    "# load the trained convolutional neural network and the label\n",
    "# binarizer\n",
    "print(\"[INFO] loading network...\")\n",
    "model = load_model(args[\"model\"])\n",
    "print(\"[INFO] loading network done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying image...\n",
      "[INFO] bulbasaur: 99.74% \n",
      "[INFO] classifying image...\n",
      "[INFO] bulbasaur: 99.96% \n",
      "[INFO] classifying image...\n",
      "[INFO] bulbasaur: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] bulbasaur: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] charmander: 97.73% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] squirtle: 80.59% \n",
      "[INFO] classifying image...\n",
      "[INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] squirtle: 99.99% \n",
      "[INFO] classifying image...\n",
      "[INFO] squirtle: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "[INFO] mewtwo: 87.34% \n"
     ]
    }
   ],
   "source": [
    "imageList = [f for f in listdir(args[\"imagefolder\"]) if isfile(join(args[\"imagefolder\"], f))]\n",
    "lb = pickle.loads(open(args[\"labelbin\"], \"rb\").read())\n",
    "    \n",
    "for filename in imageList:\n",
    "    # load the image\n",
    "    image = cv2.imread(args[\"imagefolder\"] + filename)\n",
    "    output = image.copy()\n",
    "     \n",
    "    # pre-process the image for classification\n",
    "    image = cv2.resize(image, (100, 100))\n",
    "    image = image.astype(\"float\") / 255.0\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # classify the input image\n",
    "    print(\"[INFO] classifying image...\")\n",
    "    proba = model.predict(image)[0]\n",
    "    idx = np.argmax(proba)\n",
    "    label = lb.classes_[idx]\n",
    "\n",
    "    # we'll mark our prediction as \"correct\" of the input image filename\n",
    "    # contains the predicted label text (obviously this makes the\n",
    "    # assumption that you have named your testing image files this way)\n",
    "    correct = \"\" #\"correct\" if filename.rfind(label) != -1 else \"incorrect\"\n",
    "\n",
    "    # build the label and draw the label on the image\n",
    "    label = \"{}: {:.2f}% {}\".format(label, proba[idx] * 100, correct)\n",
    "    output = imutils.resize(output, width=400)\n",
    "    cv2.putText(output, label, (10, 25),  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # show the output image\n",
    "    print(\"[INFO] {}\".format(label))\n",
    "    cv2.imshow(\"Output\", output)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "- Need images with label (from the filename)\n",
    "- Calculate classification accuracy: $acc = \\frac{N_c}{N}$\n",
    "  - $N_c$ is the number of correctly classified images.\n",
    "  - $N$ is the total number of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying image...\n",
      "bulbasaur_01.png [INFO] bulbasaur: 99.74% \n",
      "[INFO] classifying image...\n",
      "bulbasaur_02.png [INFO] bulbasaur: 99.96% \n",
      "[INFO] classifying image...\n",
      "bulbasaur_03.png [INFO] bulbasaur: 100.00% \n",
      "[INFO] classifying image...\n",
      "bulbasaur_04.png [INFO] bulbasaur: 100.00% \n",
      "[INFO] classifying image...\n",
      "charmander_05.png [INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "charmander_06.png [INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "charmander_07.png [INFO] charmander: 97.73% \n",
      "[INFO] classifying image...\n",
      "charmander_20.png [INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "charmander_21.png [INFO] charmander: 100.00% \n",
      "[INFO] classifying image...\n",
      "mewtwo_08.png [INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "mewtwo_09.png [INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "mewtwo_10.png [INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "mewtwo_11.png [INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "mewtwo_12.png [INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "mewtwo_22.png [INFO] mewtwo: 100.00% \n",
      "[INFO] classifying image...\n",
      "pikachu_13.png [INFO] squirtle: 80.59% \n",
      "[INFO] classifying image...\n",
      "pikachu_14.png [INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "pikachu_15.png [INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "pikachu_16.png [INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "pikachu_17.png [INFO] pikachu: 100.00% \n",
      "[INFO] classifying image...\n",
      "squirtle_18.png [INFO] squirtle: 99.99% \n",
      "[INFO] classifying image...\n",
      "squirtle_19.png [INFO] squirtle: 100.00% \n",
      "[INFO] classifying image...\n",
      "squirtle_23.png [INFO] mewtwo: 87.34% \n",
      "Accuracy: 0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "args[\"imagefolder\"] = \"test_samples/\"\n",
    "\n",
    "imageList = [f for f in listdir(args[\"imagefolder\"]) if isfile(join(args[\"imagefolder\"], f))]\n",
    "\n",
    "correctNo = 0\n",
    "wrongNo = 0\n",
    "\n",
    "for filename in imageList:\n",
    "    # load the image\n",
    "    image = cv2.imread(args[\"imagefolder\"] + filename)\n",
    "    output = image.copy()\n",
    "     \n",
    "    # pre-process the image for classification\n",
    "    image = cv2.resize(image, (100, 100))\n",
    "    image = image.astype(\"float\") / 255.0\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # classify the input image\n",
    "    print(\"[INFO] classifying image...\")\n",
    "    proba = model.predict(image)[0]\n",
    "    idx = np.argmax(proba)\n",
    "    label = lb.classes_[idx]\n",
    "\n",
    "    if label == filename[:filename.find('_')]:\n",
    "        correctNo += 1\n",
    "    else:\n",
    "        wrongNo += 1\n",
    "\n",
    "    # we'll mark our prediction as \"correct\" of the input image filename\n",
    "    # contains the predicted label text (obviously this makes the\n",
    "    # assumption that you have named your testing image files this way)\n",
    "    # filename = args[\"image\"][args[\"image\"].rfind(os.path.sep) + 1:]\n",
    "    correct = \"\" #\"correct\" if filename.rfind(label) != -1 else \"incorrect\"\n",
    "\n",
    "    # build the label and draw the label on the image\n",
    "    label = \"{}: {:.2f}% {}\".format(label, proba[idx] * 100, correct)\n",
    "    output = imutils.resize(output, width=400)\n",
    "    cv2.putText(output, label, (10, 25),  cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # show the output image\n",
    "    print(filename, \"[INFO] {}\".format(label))\n",
    "    #cv2.imshow(\"Output\", output)\n",
    "    #cv2.waitKey(0)\n",
    "\n",
    "print(\"Accuracy:\", correctNo/(correctNo+wrongNo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References:\n",
    "-  A. Gulli and S. Pal, Deep Learning with Keras: Implementation Neural Networks with Keras on Theano and TensorFlow, Packt Publishing, 2017.\n",
    "-  <https://keras.io/>\n",
    "-  <https://matplotlib.org/>\n",
    "-  <http://yann.lecun.com/exdb/mnist/>\n",
    "-  <https://www.kaggle.com/zalando-research/fashionmnist/home>\n",
    "-  <https://www.cs.toronto.edu/~kriz/cifar.html>\n",
    "-  I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning, MIT Press, 2016.\n",
    "-  N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: A simple way to prevent neural networks from overfitting,\" Journal of Machine Learning Research, vol.15, pp.1929--1958, 2014."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
